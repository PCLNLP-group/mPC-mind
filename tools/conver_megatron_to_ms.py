from collections import OrderedDict
import numpy as np
from pathlib import Path
import torch
import sys
import os

MAIN_DIR = os.path.abspath(
    os.path.join(
        os.path.join(
            os.path.dirname(__file__), os.path.pardir
        ), os.path.pardir
    )
)
sys.path.append(MAIN_DIR)


# The pt file in ckpt_path is generated by
# examples/pretrain_gpt_rope_7b_test.sh
ckpt_path = Path(
    '/userhome/model/wanli/pretrain-wanli-7B-0820/merger_1/iter_0024019/mp_rank_00/model_optim_rng.pt')

ms_npy_base_path = Path('/userhome/tmp/7B-megatron-to-npy')
ms_npy_base_path.mkdir(exist_ok=True, parents=True)
npy_save_path = ms_npy_base_path / 'merged_ckpt.npy'


# load pt
pt_dict = torch.load(ckpt_path, map_location=torch.device('cpu'))


args_7B = {'bf16': False,
 'fp16': True,
 'num_layers': 32,
 'encoder_num_layers': 32,
 'seq_length': 4096,
 'encoder_seq_length': 4096,
 'max_position_embeddings': 4096,
 'hidden_size': 4096,
 'ffn_hidden_size': 10880,
 'make_vocab_size_divisible_by': 128,
 'micro_batch_size': 1,
 'num_attention_heads': 32,
 'use_rotary_position_embeddings': True,
 'ms_fast_gelu': False,
 'add_position_embedding':False,
 'padded_vocab_size': 125952}

args_200B = {'bf16': False,
 'fp16': True,
 'num_layers': 103,
 'encoder_num_layers': 103,
 'seq_length': 2048,
 'encoder_seq_length': 2048,
 'max_position_embeddings': 2048,
 'hidden_size': 12672,
 'ffn_hidden_size': 50688,
 'make_vocab_size_divisible_by': 64,
 'micro_batch_size': 1,
 'num_attention_heads': 96,
 'use_rotary_position_embeddings': True,
 'ms_fast_gelu': True,
 'add_position_embedding':False,
 'padded_vocab_size': 49984}

args = args_7B

for name in args:
    setattr(pt_dict['args'], name, args[name])

print(pt_dict['args'])
print(pt_dict['model']['language_model']['encoder'].keys())

mgt_language_model = pt_dict['model']['language_model']

# npy_data = np.load(ms_npy_base_path / 'merged_ckpt.npy', allow_pickle=True)

npy_dict = {}
num_head = args['num_attention_heads']
hidden_size = args['hidden_size']
hp = hidden_size // num_head

mgt_encoder = mgt_language_model['encoder']
for layer_i in range(args['num_layers']):
    npy_dict[f"backbone.blocks.{layer_i}.layernorm1.gamma"] = mgt_encoder[f'layers.{layer_i}.input_layernorm.weight']
    npy_dict[f"backbone.blocks.{layer_i}.layernorm1.beta"] = mgt_encoder[f'layers.{layer_i}.input_layernorm.bias']

    qkv_w = mgt_encoder[f'layers.{layer_i}.self_attention.query_key_value.weight']
    q_w_restored, k_w_restored, v_w_restored = torch.split(qkv_w.view(*[-1, hp, 3*hidden_size]), hidden_size, dim=-1)
    npy_dict[f"backbone.blocks.{layer_i}.attention.dense1.weight"] = q_w_restored.view(hidden_size, hidden_size)
    npy_dict[f"backbone.blocks.{layer_i}.attention.dense2.weight"] = k_w_restored.view(hidden_size, hidden_size)
    npy_dict[f"backbone.blocks.{layer_i}.attention.dense3.weight"] = v_w_restored.view(hidden_size, hidden_size)

    qkv_b = mgt_encoder[f'layers.{layer_i}.self_attention.query_key_value.bias']
    q_b,k_b,v_b = torch.split(qkv_b.view(*[-1, 3*hp]),hp,dim=-1)
    q_b.stride()
    npy_dict[f"backbone.blocks.{layer_i}.attention.dense1.bias"] = q_b.reshape(hidden_size)
    npy_dict[f"backbone.blocks.{layer_i}.attention.dense2.bias"] = k_b.reshape(hidden_size)
    npy_dict[f"backbone.blocks.{layer_i}.attention.dense3.bias"] = v_b.reshape(hidden_size)

    npy_dict[f"backbone.blocks.{layer_i}.attention.projection.weight"] = mgt_encoder[f'layers.{layer_i}.self_attention.dense.weight'].T
    npy_dict[f"backbone.blocks.{layer_i}.attention.projection.bias"] = mgt_encoder[f'layers.{layer_i}.self_attention.dense.bias']
    npy_dict[f"backbone.blocks.{layer_i}.layernorm2.gamma"] = mgt_encoder[f'layers.{layer_i}.post_attention_layernorm.weight']
    npy_dict[f"backbone.blocks.{layer_i}.layernorm2.beta"] = mgt_encoder[f'layers.{layer_i}.post_attention_layernorm.bias']
    npy_dict[f"backbone.blocks.{layer_i}.output.mapping.weight"] = mgt_encoder[f'layers.{layer_i}.mlp.dense_h_to_4h.weight'].T
    npy_dict[f"backbone.blocks.{layer_i}.output.mapping.bias"] = mgt_encoder[f'layers.{layer_i}.mlp.dense_h_to_4h.bias']
    npy_dict[f"backbone.blocks.{layer_i}.output.projection.weight"] = mgt_encoder[f'layers.{layer_i}.mlp.dense_4h_to_h.weight'].T
    npy_dict[f"backbone.blocks.{layer_i}.output.projection.bias"] = mgt_encoder[f'layers.{layer_i}.mlp.dense_4h_to_h.bias']

npy_dict["backbone.layernorm.gamma"] = mgt_encoder[f'final_layernorm.weight']
npy_dict["backbone.layernorm.beta"] = mgt_encoder[f'final_layernorm.bias']

npy_dict["backbone.embedding.word_embedding.embedding_table"] = mgt_language_model['embedding']['word_embeddings']['weight']
npy_dict["backbone.embedding.position_embedding.embedding_table"] = mgt_language_model['embedding']['position_embeddings']['weight']

npy_dict_npy = []
for i in npy_dict:
    npy_dict_npy.append({'name':i, 'data':npy_dict[i]}.numpy())

np.save(npy_save_path, npy_dict_npy)

print('finish')
